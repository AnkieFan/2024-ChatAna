{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "不想让某个字/词语参与分析：在[stopwords.txt](stopwords.txt)里添加这个字/词  \n",
    "想自定义某个词语：在[self_dict.txt](stopwords.txt)里添加这个词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "Type = {\n",
    "    '1':'文字',\n",
    "    '3':'图片',\n",
    "    '43':'视频',\n",
    "    '-1879048185':'微信运动排行榜',\n",
    "    '5':'',\n",
    "    '47':'表情包',\n",
    "    '268445456':'撤回的消息',\n",
    "    '34':'语音',\n",
    "    '419430449':'转账',\n",
    "    '50':'语音电话',\n",
    "    '10000':'进群、撤回、拒收等系统消息',\n",
    "    '822083633':'回复消息',\n",
    "    '922746929':'拍一拍',\n",
    "    '1090519089':'发送文件',\n",
    "    '318767153':'付款成功',\n",
    "    '436207665':'发红包',\n",
    "}\n",
    "\n",
    "jieba.load_userdict('self_dict.txt')\n",
    "df = pd.read_csv('db_tables\\messages.csv') # TODO: 改成你留痕导出的csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = df['Type'].astype(str).map(Type)\n",
    "type_counts = df.groupby(['Type', 'IsSender']).size().reset_index(name='Count')\n",
    "type_pivot = type_counts.pivot_table(index='Type', columns='IsSender', values='Count', fill_value=0)\n",
    "type_pivot.columns = ['Received', 'Sent']\n",
    "type_pivot.reset_index(inplace=True)\n",
    "print('你的所有聊天总览：')\n",
    "type_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "df = df[df['StrTime'] > '2024-01-01 00:00:00']\n",
    "\n",
    "type_counts = df.groupby(['Type', 'IsSender']).size().reset_index(name='Count')\n",
    "type_pivot = type_counts.pivot_table(index='Type', columns='IsSender', values='Count', fill_value=0)\n",
    "type_pivot.columns = ['Received', 'Sent']\n",
    "type_pivot.reset_index(inplace=True)\n",
    "print('你的2024聊天总览：')\n",
    "type_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_chinese(text:str):\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# 清洗文本数据\n",
    "def clean(text): #返回清洗后的句子\n",
    "    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    if re.match(url_pattern, text): return ''\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) #英文符号\n",
    "    if(not contains_chinese(text)): return text\n",
    "       \n",
    "    text = text.rstrip()\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('·', ' ')\n",
    "    text = text.replace('...', '…')\n",
    "    punctuation = r\"[\\u3000-\\u303F\\uff00-\\uffef]|[!?\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~]\" #中文符号\n",
    "    text = re.sub(punctuation, ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text) #多余的空格\n",
    "    return text\n",
    "\n",
    "# 分词：\n",
    "def tokenize(text:str):\n",
    "    if(not contains_chinese(text)): return [str(text).strip()]\n",
    "    if(text == None or len(text) < 1): return ''\n",
    "\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "        stopwords = {line.strip() for line in file}\n",
    "\n",
    "    words = jieba.cut(text)\n",
    "    clean_words = [word for word in words if word not in stopwords and word.strip() != '']\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_df = df[['IsSender', 'Type', 'StrTime', 'StrContent', 'Remark', 'NickName']]\n",
    "pro_df = pro_df[pro_df['Type'] == '文字'] # 之后只分析文字消息\n",
    "pro_df.drop(columns=['Type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成聊天联系人文件用来手动删掉群聊的联系人\n",
    "unique_contacts = pro_df[['NickName']].drop_duplicates()\n",
    "unique_contacts.to_csv('unique_contacts.csv', index=False)\n",
    "# 不包括在分析的联系人：\n",
    "groups = ['文件传输助手'] # TODO: 自己加 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删掉这些 还有我发少于20条的联系人\n",
    "pro_df = pro_df[~pro_df['NickName'].isin(groups)]\n",
    "message_counts = pro_df[pro_df['IsSender'] == 1].groupby('NickName').size()\n",
    "talkers_to_keep = message_counts[message_counts >= 20].index\n",
    "pro_df = pro_df[pro_df['NickName'].isin(talkers_to_keep)]\n",
    "pro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本清洗\n",
    "pro_df['CleanedContent'] = pro_df['StrContent'].apply(clean)\n",
    "pro_df['TokenizedContent'] = pro_df['CleanedContent'].apply(tokenize)\n",
    "pro_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些全部统计：\n",
    "all_send_messages = pro_df[pro_df['IsSender'] == 1]['StrContent'].tolist()\n",
    "send_characters = sum(len(message) for message in all_send_messages)\n",
    "\n",
    "all_received_messages = pro_df[pro_df['IsSender'] == 0]['StrContent'].tolist()\n",
    "received_characters = sum(len(message) for message in all_received_messages)\n",
    "\n",
    "print(f'今年你一共发出了{len(all_send_messages)}条文本消息，收到了{len(all_received_messages)}条文本消息。')\n",
    "print(f'一共发送了{send_characters}个字符，收到了{received_characters}个字符。\\n')\n",
    "\n",
    "# 聊天总数排名：\n",
    "top_chat = pro_df.groupby('NickName').agg(\n",
    "    MessageCount=('NickName', 'size'),       # Count the number of messages\n",
    "    Remark=('Remark', 'first')               # Get the first 'Remark' for each 'NickName'\n",
    ").reset_index()\n",
    "top_chat.sort_values(by='MessageCount', ascending=False, inplace=True)\n",
    "print(f'其中你和{top_chat.iloc[0][\"NickName\"]}（备注：{top_chat.iloc[0][\"Remark\"]}）聊得最多，一共有{top_chat.iloc[0][\"MessageCount\"]}条消息。')\n",
    "print(f'其次是{top_chat.iloc[1][\"NickName\"]}（备注：{top_chat.iloc[1][\"Remark\"]}），一共有{top_chat.iloc[1][\"MessageCount\"]}条消息。')\n",
    "print(f'接着是{top_chat.iloc[2][\"NickName\"]}（备注：{top_chat.iloc[2][\"Remark\"]}），一共有{top_chat.iloc[2][\"MessageCount\"]}条消息。\\n')\n",
    "\n",
    "\n",
    "# 发送消息最多的\n",
    "top_senders = pro_df[pro_df['IsSender'] == 1].groupby('NickName').size().sort_values(ascending=False)\n",
    "print(f'你发送消息最多的联系人是{top_senders.index[0]}，一共发送了{top_senders[0]}条消息。')\n",
    "print(f'其次是{top_senders.index[1]}，一共发送了{top_senders[1]}条消息。')\n",
    "print(f'接着是{top_senders.index[2]}，一共发送了{top_senders[2]}条消息。\\n')\n",
    "\n",
    "\n",
    "# 接收消息最多的\n",
    "top_receivers = pro_df[pro_df['IsSender'] == 0].groupby('NickName').size().sort_values(ascending=False)\n",
    "print(f'你接收消息最多的联系人是{top_receivers.index[0]}，一共收到了{top_receivers[0]}条消息。')\n",
    "print(f'其次是{top_receivers.index[1]}，一共收到了{top_receivers[1]}条消息。')\n",
    "print(f'接着是{top_receivers.index[2]}，一共收到了{top_receivers[2]}条消息。\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词频统计\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def word_frequency(df):\n",
    "    wc = WordCloud(font_path='msyh.ttc', background_color='white')\n",
    "    word_counter = Counter()\n",
    "    for token_list in pro_df['TokenizedContent']:\n",
    "        if len(token_list) < 1 or (len(token_list) == 1 and len(token_list[0]) <= 1): continue\n",
    "        word_counter.update(token_list)\n",
    "    \n",
    "    wc.generate_from_frequencies(word_counter)\n",
    "    plt.figure(figsize=(10, 5))  # Set the figure size\n",
    "    plt.imshow(wc, interpolation='bilinear')  # Display the word cloud\n",
    "    plt.axis(\"off\")  # Do not show axes to keep it clean\n",
    "    plt.show() \n",
    "\n",
    "    return word_counter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_count = word_frequency(pro_df)\n",
    "count_xs = all_word_count['想死']\n",
    "print(f'我在2024说了{count_xs}次想死')\n",
    "count_bb = all_word_count['拔杯']\n",
    "print(f'说了{count_bb}次拔杯')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按联系人："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_dfs = {}\n",
    "for nickname, group_df in pro_df.groupby('NickName'):\n",
    "    contact_dfs[nickname] = group_df\n",
    "\n",
    "contact_dfs[\"someone's nickname\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency(contact_dfs[\"someone's nickname\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
